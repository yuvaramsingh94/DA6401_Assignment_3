{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from utils import decoder_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from utils import str_idx_to_list\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_df: pd.DataFrame,\n",
    "        X_max_length: int,\n",
    "        Y_max_length: int,\n",
    "        X_vocab_size: int,\n",
    "        Y_vocab_size: int,\n",
    "    )-> tuple(torch.tensor,torch.tensor,torch.tensor,torch.tensor,torch.tensor,torch.tensor):\n",
    "        \"\"\"\n",
    "        Text dataset for translit from English to Tamil task.\n",
    "\n",
    "        Args:\n",
    "            dataset_df (pd.DataFrame): _description_\n",
    "            X_max_length (int): Maximum length of the word (in X) in the dataset\n",
    "            Y_max_length (int): Maximum length of the word (in Y) in the dataset\n",
    "            X_vocab_size (int): Size of the X vocabulary. This is to add padding integer.\n",
    "            Y_vocab_size (int): Size of the Y vocabulary. This is to add padding integer.\n",
    "        \"\"\"\n",
    "        self.dataset_df = dataset_df\n",
    "        self.X_max_length = X_max_length\n",
    "        self.Y_max_length = Y_max_length\n",
    "        self.X_vocab_size = X_vocab_size\n",
    "        self.Y_vocab_size = Y_vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        X = str_idx_to_list(self.dataset_df.iloc[idx][\"English\"])  # .values[0]\n",
    "        Y = str_idx_to_list(self.dataset_df.iloc[idx][\"Tamil\"])  # .values[0]\n",
    "\n",
    "        ## Decoder input y\n",
    "        Y_decoder_ip = Y[:-1]\n",
    "        ## Decoder output y\n",
    "        Y_decoder_op = Y[1:]\n",
    "        ## The actual length of the sequence\n",
    "        X_len = len(X)\n",
    "        Y_decoder_ip_len = len(Y_decoder_ip)\n",
    "        Y_decoder_op_len = len(Y_decoder_op)\n",
    "        if X_len < self.X_max_length:\n",
    "            ## self.X_vocab_size refer to the padding index (last)\n",
    "            X.extend([self.X_vocab_size] * (self.X_max_length - X_len))\n",
    "        ## Decoder IP\n",
    "        if Y_decoder_ip_len < self.Y_max_length:\n",
    "            ## self.Y_vocab_size refer to the padding index (last)\n",
    "            Y_decoder_ip.extend(\n",
    "                [self.Y_vocab_size] * (self.Y_max_length - Y_decoder_ip_len)\n",
    "            )\n",
    "        if Y_decoder_op_len < self.Y_max_length:\n",
    "            ## self.Y_vocab_size refer to the padding index (last)\n",
    "            Y_decoder_op.extend(\n",
    "                [self.Y_vocab_size] * (self.Y_max_length - Y_decoder_op_len)\n",
    "            )\n",
    "        ## Padding index\n",
    "        ## X : English\n",
    "        ## Y : Tamil\n",
    "        \n",
    "        \n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        Y_decoder_ip = torch.tensor(Y_decoder_ip, dtype=torch.long)\n",
    "        Y_decoder_op = torch.tensor(Y_decoder_op, dtype=torch.long)\n",
    "\n",
    "        X_len = torch.tensor(X_len, dtype=torch.long)\n",
    "        Y_decoder_ip_len = torch.tensor(Y_decoder_ip_len, dtype=torch.long)\n",
    "        Y_decoder_op_len = torch.tensor(Y_decoder_op_len, dtype=torch.long)\n",
    "        return X, Y_decoder_ip, Y_decoder_op, X_len, Y_decoder_ip_len, Y_decoder_op_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "DATASET_PATH = os.path.join(\"..\",\"dataset\",\"dakshina_dataset_v1.0\",\"ta\",\"lexicons\")\n",
    "TRAIN = \"ta.translit.sampled.train.tsv\"\n",
    "VAL = \"ta.translit.sampled.dev.tsv\"\n",
    "TEST =   \"ta.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaiyaalam--------------------\n",
      "\tஅடையாளம்-----------------\n",
      "அடையாளம்\n",
      "-----------------\n",
      "30\n",
      "26\n",
      "26\n",
      "lengths\n",
      "tensor(10) tensor(9) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "# x,y_dec_ip,y_dec_op,X_len, Y_decoder_ip_len, Y_decoder_op_len = train_dataset.__getitem__(867)\n",
    "# print(decoder_function(character_idx_seq=','.join([str(i) for i in x.detach().tolist()]), idx_to_char_dict=english_idx_to_char))\n",
    "# print(decoder_function(character_idx_seq=','.join([str(i) for i in y_dec_ip.detach().tolist()]), idx_to_char_dict=tamil_idx_to_char))\n",
    "# print(decoder_function(character_idx_seq=','.join([str(i) for i in y_dec_op.detach().tolist()]), idx_to_char_dict=tamil_idx_to_char))\n",
    "\n",
    "# print(x.__len__())\n",
    "# print(y_dec_ip.__len__())\n",
    "# print(y_dec_op.__len__())\n",
    "# print(\"lengths\")\n",
    "# print(X_len, Y_decoder_ip_len, Y_decoder_op_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the RNN network\n",
    "from lightning import LightningModule\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "## TODO: use torch.nn.utils.rnn.pack_padded_sequence\n",
    "## TODO: torch.nn.utils.rnn.pad_packed_sequence\n",
    "\n",
    "\n",
    "class EncoderNetwork(LightningModule):\n",
    "    def __init__(self, config: dict):\n",
    "        super(EncoderNetwork, self).__init__()\n",
    "        self.config = config\n",
    "        ## encoder\n",
    "        ### Embedding layer\n",
    "        #### Here config.X_vocab_size will be the padding index also\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.config.X_vocab_size,\n",
    "            self.config.encoder_embedding_size,\n",
    "            padding_idx=self.config.X_padding_idx,\n",
    "        )\n",
    "\n",
    "        if self.config.recurrent_layer_type == \"RNN\":\n",
    "            self.recursive_layer = nn.RNN(\n",
    "                input_size=self.config.encoder_embedding_size,\n",
    "                hidden_size=self.config.encoder_hidden_size,\n",
    "                num_layers=self.config.num_encoder_layers,\n",
    "                dropout=self.config.encoder_dropout_prob,\n",
    "                bidirectional=self.config.encoder_bidir,\n",
    "                nonlinearity=self.config.encoder_nonlinearity,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        ## Initialize H0\n",
    "        ##! The doc said the H0 will dafault to zeros. Going to check this https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n",
    "        e_x = self.embedding(x)\n",
    "\n",
    "        ## Pack the padded input for better computation\n",
    "        packed = pack_padded_sequence(e_x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        output_packed, h_n = self.recursive_layer(packed)\n",
    "\n",
    "        output, _ = pad_packed_sequence(output_packed, batch_first=True, total_length = self.config.X_max_length)\n",
    "        return e_x, output, h_n\n",
    "\n",
    "\n",
    "class DecoderNetwork(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super(DecoderNetwork, self).__init__()\n",
    "        self.config = config\n",
    "        ## Tamil encoder\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.Y_vocab_size,\n",
    "            embedding_dim=config.decoder_embedding_size,\n",
    "            padding_idx=config.Y_padding_idx,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=config.decoder_embedding_size,\n",
    "            hidden_size=config.decoder_hidden_size,\n",
    "            num_layers=config.num_decoder_layers,\n",
    "            dropout=config.decoder_dropout_prob,\n",
    "            bidirectional=config.decoder_bidir,\n",
    "            nonlinearity=config.decoder_nonlinearity,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        ## Here the Vocab size should be one less as we added 1 for the embedding layer\n",
    "        self.fc = nn.Linear(config.decoder_hidden_size, config.Y_vocab_size-1)\n",
    "\n",
    "    def forward(self, y_decoder_input, encoder_hidden):\n",
    "        \"\"\"\n",
    "        y_decoder_input: (batch, tgt_seq_len)\n",
    "        encoder_hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(y_decoder_input)  # (batch, tgt_seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(embedded, encoder_hidden)\n",
    "        # output: (batch, tgt_seq_len, hidden_size)\n",
    "        logits = self.fc(output)  # (batch, tgt_seq_len, vocab_size)\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30])\n",
      "torch.Size([2, 26])\n",
      "torch.Size([2, 26])\n",
      "tensor([10,  5]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# #from RecursiveNetwork import EncoderNetwork, DecoderNetwork\n",
    "# train_iter = iter(train_loader)\n",
    "# x,y_dec_ip,y_dec_op, x_len,_,_ = next(train_iter)\n",
    "\n",
    "# print(x.shape)\n",
    "# print(y_dec_ip.shape)\n",
    "# print(y_dec_op.shape)\n",
    "# print(x_len, x_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.wandb_project = \"assignment_3\"\n",
    "        self.wandb_entity = \"v1\"\n",
    "        self.epoch = 5\n",
    "        self.batch_size = 16\n",
    "        self.encoder_embedding_size = 128\n",
    "        self.X_vocab_size = 26+1 ## Here +1 is because the nn.Embedding layer throws this error AssertionError =  Padding_idx must be within num_embeddings\n",
    "        self.X_padding_idx = 26\n",
    "        self.X_max_length = 30\n",
    "        self.Y_max_length = 26\n",
    "        self.recurrent_layer_type = \"RNN\"\n",
    "        self.encoder_hidden_size = 256\n",
    "        self.num_encoder_layers = 1\n",
    "        self.encoder_dropout_prob = 0.0\n",
    "        self.encoder_bidir = False\n",
    "        self.encoder_nonlinearity = \"tanh\"\n",
    "        self.Y_vocab_size = 48+1 ## Here +1 is because the nn.Embedding layer throws this error AssertionError =  Padding_idx must be within num_embeddings\n",
    "        self.decoder_embedding_size = 128\n",
    "        self.Y_padding_idx = 48\n",
    "        self.decoder_hidden_size = 256\n",
    "        self.num_decoder_layers = 1\n",
    "        self.decoder_dropout_prob = 0.0\n",
    "        self.decoder_bidir = False\n",
    "        self.decoder_nonlinearity = \"tanh\"\n",
    "        self.Y_true_vocab_size = 48 ## No need for extra digit for padding s required by nn.Embedding\n",
    "        self.LR = 1e-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config()\n",
    "# RNN_enc_net = EncoderNetwork(config = config)\n",
    "# RNN_dec_net = DecoderNetwork(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_, out, h_out = RNN_enc_net.forward(x,x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out.shape, h_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, hidden = RNN_dec_net.forward(y_dec_ip,encoder_hidden=h_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 48])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from lightning import LightningModule\n",
    "from lightning import Trainer, seed_everything\n",
    "\n",
    "SEED = 5\n",
    "seed_everything(SEED, workers=True)\n",
    "\n",
    "class Seq2SeqModel(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoder = EncoderNetwork(self.config)\n",
    "        self.decoder = DecoderNetwork(self.config)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=self.config.Y_padding_idx  # Mask out padding positions\n",
    "        )\n",
    "        \n",
    "        self.train_correct = 0\n",
    "        self.train_total = 0\n",
    "        self.val_correct = 0\n",
    "        self.val_total = 0\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def forward(self, x, X_len, y_dec_ip):\n",
    "        # Encoder forward (optionally use X_len for packing)\n",
    "        _, _, encoder_hidden = self.encoder(x, X_len)\n",
    "        # Decoder forward\n",
    "        logits, _ = self.decoder(y_dec_ip, encoder_hidden)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        \n",
    "        x,y_dec_ip,y_dec_op,X_len,_,_ = batch  \n",
    "        \n",
    "        logits = self(x, X_len, y_dec_ip)  # (batch, tgt_len, vocab_size)\n",
    "        ## reshaping to match the required shape of (N,C) for logits \n",
    "        ## and (N,) for label\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = y_dec_op.view(-1) ## Flatten the decoder \n",
    "        loss = self.loss_fn(logits, targets)\n",
    "\n",
    "        ## Accuracy and loss tracking\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(prob, dim=1)\n",
    "        correct = (preds == targets).sum().item()\n",
    "        batch_size = logits.size(0)\n",
    "\n",
    "        # Update counters\n",
    "        self.train_correct += correct\n",
    "        self.train_total += batch_size\n",
    "        self.train_loss.append(loss.view(1).cpu())\n",
    "\n",
    "\n",
    "\n",
    "        #self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        \n",
    "        x,y_dec_ip,y_dec_op,X_len,_,_ = batch  \n",
    "        \n",
    "        logits = self(x, X_len, y_dec_ip)  # (batch, tgt_len, vocab_size)\n",
    "        ## reshaping to match the required shape of (N,C) for logits \n",
    "        ## and (N,) for label\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        targets = y_dec_op.view(-1) ## Flatten the decoder \n",
    "        loss = self.loss_fn(logits, targets)\n",
    "\n",
    "        ## Accuracy and loss tracking\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(prob, dim=1)\n",
    "        correct = (preds == targets).sum().item()\n",
    "        batch_size = logits.size(0)\n",
    "\n",
    "        # Update counters\n",
    "        self.val_correct += correct\n",
    "        self.val_total += batch_size\n",
    "        self.val_loss.append(loss.view(1).cpu())\n",
    "\n",
    "\n",
    "\n",
    "        #self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # Calculate epoch accuracy\n",
    "        epoch_acc = self.train_correct / self.train_total\n",
    "        self.log(\"train_acc_epoch\", epoch_acc)\n",
    "        if len(self.train_loss) > 0:\n",
    "            self.log(\"train_loss_epoch\", torch.cat(self.train_loss).mean())\n",
    "        # Reset lists\n",
    "        self.train_correct = 0\n",
    "        self.train_total = 0\n",
    "        self.train_loss = []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Calculate epoch accuracy\n",
    "        epoch_acc = self.val_correct / self.val_total\n",
    "        self.log(\"val_acc_epoch\", epoch_acc)\n",
    "        if len(self.val_loss) > 0:\n",
    "            self.log(\"val_loss_epoch\", torch.cat(self.val_loss).mean())\n",
    "        # Reset lists\n",
    "        self.val_correct = 0\n",
    "        self.val_total = 0\n",
    "        self.val_loss = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.LR)\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": ReduceLROnPlateau(\n",
    "                optimizer=optimizer, mode=\"max\", factor=0.1, patience=2\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"monitor\": \"val_acc_epoch\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated, thus stopping\n",
    "            # training if not found. If set to `False`, it will only produce a warning\n",
    "            \"strict\": True,\n",
    "            \"name\": \"LR_track\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATASET_PATH, \"ta.translit.sampled.train.idx.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(DATASET_PATH, \"ta.translit.sampled.dev.idx.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATASET_PATH, \"ta.translit.sampled.test.idx.csv\"))\n",
    "\n",
    "\n",
    "with open(os.path.join(DATASET_PATH,\"tamil_token_index.json\"),'r', encoding=\"utf-8\") as f:\n",
    "\ttamil_idx = json.load(f)\n",
    "tamil_idx_to_char = {j:i for i,j in tamil_idx.items()}\n",
    "\n",
    "with open(os.path.join(DATASET_PATH,\"english_token_index.json\"),'r', encoding=\"utf-8\") as f:\n",
    "\tenglish_idx = json.load(f)\n",
    "english_idx_to_char = {j:i for i,j in english_idx.items()}\n",
    "\n",
    "## For padding\n",
    "english_idx_to_char[26] = \"-\"\n",
    "tamil_idx_to_char[48] = \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68218, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomTextDataset(dataset_df = test_df, X_max_length = 30,\n",
    "        Y_max_length = 27,\n",
    "        X_vocab_size = 26,\n",
    "        Y_vocab_size = 48,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(dataset.__len__()):\n",
    "    X, Y_decoder_ip, Y_decoder_op, X_len, Y_decoder_ip_len, Y_decoder_op_len = dataset.__getitem__(i)\n",
    "    if Y_decoder_ip.shape[0] != 27:\n",
    "        print(\"idx\",i,\"Y_decoder_ip\",Y_decoder_ip.shape[0] )\n",
    "    if Y_decoder_op.shape[0] != 27:\n",
    "        print(\"idx\",i,\"Y_decoder_op\",Y_decoder_op.shape[0] )\n",
    "    if X.shape[0] != 30:\n",
    "        print(\"idx\",i,\"X\" )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train 38766 38767 Y_decoder_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y_decoder_ip, Y_decoder_op, X_len, Y_decoder_ip_len, Y_decoder_op_len = train_dataset.__getitem__(38766)\n",
    "Y_decoder_op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = CustomTextDataset(dataset_df = train_df[:200], X_max_length = 30,\n",
    "        Y_max_length = 26,\n",
    "        X_vocab_size = 26,\n",
    "        Y_vocab_size = 48,)\n",
    "\n",
    "val_dataset = CustomTextDataset(dataset_df = val_df[:200], X_max_length = 30,\n",
    "        Y_max_length = 26,\n",
    "        X_vocab_size = 26,\n",
    "        Y_vocab_size = 48,)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    # num_workers=2,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True,\n",
    "    # num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lit_model = Seq2SeqModel(config=config)\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "        project=config.wandb_project,\n",
    "        name=config.wandb_entity,\n",
    "        log_model=False,\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | encoder | EncoderNetwork   | 102 K  | train\n",
      "1 | decoder | DecoderNetwork   | 117 K  | train\n",
      "2 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "219 K     Trainable params\n",
      "0         Non-trainable params\n",
      "219 K     Total params\n",
      "0.879     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuvar\\miniconda3\\envs\\fastapi\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:00<00:00, 15.39it/s, v_num=b9ex]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 12/12 [00:00<00:00, 14.64it/s, v_num=b9ex]\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "trainer = Trainer(\n",
    "    max_epochs=config.epoch,\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=100,\n",
    "    logger=wandb_logger,\n",
    "    #callbacks=[checkpoint_callback],\n",
    ")  # Added accelerator gpu, can be cpu also, devices set to 1\n",
    "\n",
    "trainer.fit(\n",
    "    lit_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
