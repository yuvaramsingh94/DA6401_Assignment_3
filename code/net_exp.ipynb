{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from utils import decoder_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.v2 import Normalize\n",
    "from torchvision.transforms import Resize\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from utils import str_idx_to_list\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_df: pd.DataFrame,\n",
    "        X_max_length: int,\n",
    "        Y_max_length: int,\n",
    "        X_vocab_size: int,\n",
    "        Y_vocab_size: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Text dataset for translit from English to Tamil task.\n",
    "\n",
    "        Args:\n",
    "            dataset_df (pd.DataFrame): _description_\n",
    "            X_max_length (int): Maximum length of the word (in X) in the dataset\n",
    "            Y_max_length (int): Maximum length of the word (in Y) in the dataset\n",
    "            X_vocab_size (int): Size of the X vocabulary. This is to add padding integer.\n",
    "            Y_vocab_size (int): Size of the Y vocabulary. This is to add padding integer.\n",
    "        \"\"\"\n",
    "        self.dataset_df = dataset_df\n",
    "        self.X_max_length = X_max_length\n",
    "        self.Y_max_length = Y_max_length\n",
    "        self.X_vocab_size = X_vocab_size\n",
    "        self.Y_vocab_size = Y_vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        X = str_idx_to_list(self.dataset_df.iloc[idx][\"English\"])  # .values[0]\n",
    "        Y = str_idx_to_list(self.dataset_df.iloc[idx][\"Tamil\"])  # .values[0]\n",
    "\n",
    "        ## Decoder input y\n",
    "        Y_decoder_ip = Y[:-1]\n",
    "        ## Decoder output y\n",
    "        Y_decoder_op = Y[1:]\n",
    "        ## The actual length of the sequence\n",
    "        X_len = len(X)\n",
    "        Y_decoder_ip_len = len(Y_decoder_ip)\n",
    "        Y_decoder_op_len = len(Y_decoder_op)\n",
    "        if X_len < self.X_max_length:\n",
    "            ## self.X_vocab_size refer to the padding index (last)\n",
    "            X.extend([self.X_vocab_size] * (self.X_max_length - X_len))\n",
    "        ## Decoder IP\n",
    "        if Y_decoder_ip_len < self.Y_max_length:\n",
    "            ## self.Y_vocab_size refer to the padding index (last)\n",
    "            Y_decoder_ip.extend(\n",
    "                [self.Y_vocab_size] * (self.Y_max_length - Y_decoder_ip_len)\n",
    "            )\n",
    "        if Y_decoder_op_len < self.Y_max_length:\n",
    "            ## self.Y_vocab_size refer to the padding index (last)\n",
    "            Y_decoder_op.extend(\n",
    "                [self.Y_vocab_size] * (self.Y_max_length - Y_decoder_op_len)\n",
    "            )\n",
    "        ## Padding index\n",
    "        ## X : English\n",
    "        ## Y : Tamil\n",
    "        \n",
    "        \n",
    "\n",
    "        X = torch.tensor(X, dtype=torch.long)\n",
    "        Y_decoder_ip = torch.tensor(Y_decoder_ip, dtype=torch.long)\n",
    "        Y_decoder_op = torch.tensor(Y_decoder_op, dtype=torch.long)\n",
    "\n",
    "        X_len = torch.tensor(X_len, dtype=torch.long)\n",
    "        Y_decoder_ip_len = torch.tensor(Y_decoder_ip_len, dtype=torch.long)\n",
    "        Y_decoder_op_len = torch.tensor(Y_decoder_op_len, dtype=torch.long)\n",
    "        return X, Y_decoder_ip, Y_decoder_op, X_len, Y_decoder_ip_len, Y_decoder_op_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "DATASET_PATH = os.path.join(\"..\",\"dataset\",\"dakshina_dataset_v1.0\",\"ta\",\"lexicons\")\n",
    "TRAIN = \"ta.translit.sampled.train.tsv\"\n",
    "VAL = \"ta.translit.sampled.dev.tsv\"\n",
    "TEST =   \"ta.translit.sampled.test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_idx_df = pd.read_csv(os.path.join(DATASET_PATH, \"ta.translit.sampled.train.idx.csv\"))\n",
    "\n",
    "\n",
    "with open(os.path.join(DATASET_PATH,\"tamil_token_index.json\"),'r', encoding=\"utf-8\") as f:\n",
    "\ttamil_idx = json.load(f)\n",
    "tamil_idx_to_char = {j:i for i,j in tamil_idx.items()}\n",
    "\n",
    "with open(os.path.join(DATASET_PATH,\"english_token_index.json\"),'r', encoding=\"utf-8\") as f:\n",
    "\tenglish_idx = json.load(f)\n",
    "english_idx_to_char = {j:i for i,j in english_idx.items()}\n",
    "\n",
    "## For padding\n",
    "english_idx_to_char[26] = \"-\"\n",
    "tamil_idx_to_char[48] = \"-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "text_dataset = CustomTextDataset(dataset_df = char_idx_df, X_max_length = 30,\n",
    "        Y_max_length = 26,\n",
    "        X_vocab_size = 26,\n",
    "        Y_vocab_size = 48,)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    text_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    # num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaiyaalam--------------------\n",
      "\tஅடையாளம்-----------------\n",
      "அடையாளம்\n",
      "-----------------\n",
      "30\n",
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "x,y_dec_ip,y_dec_op,X_len, Y_decoder_ip_len, Y_decoder_op_len = text_dataset.__getitem__(867)\n",
    "print(decoder_function(character_idx_seq=','.join([str(i) for i in x.detach().tolist()]), idx_to_char_dict=english_idx_to_char))\n",
    "print(decoder_function(character_idx_seq=','.join([str(i) for i in y_dec_ip.detach().tolist()]), idx_to_char_dict=tamil_idx_to_char))\n",
    "print(decoder_function(character_idx_seq=','.join([str(i) for i in y_dec_op.detach().tolist()]), idx_to_char_dict=tamil_idx_to_char))\n",
    "\n",
    "print(x.__len__())\n",
    "print(y_dec_ip.__len__())\n",
    "print(y_dec_op.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the RNN network\n",
    "from pytorch_lightning import LightningModule\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "## TODO: use torch.nn.utils.rnn.pack_padded_sequence\n",
    "## TODO: torch.nn.utils.rnn.pad_packed_sequence\n",
    "\n",
    "\n",
    "class EncoderNetwork(LightningModule):\n",
    "    def __init__(self, config: dict):\n",
    "        super(EncoderNetwork, self).__init__()\n",
    "        self.config = config\n",
    "        ## encoder\n",
    "        ### Embedding layer\n",
    "        #### Here config[\"X_vocab_size\"] will be the padding index also\n",
    "        self.embedding = nn.Embedding(\n",
    "            self.config[\"X_vocab_size\"],\n",
    "            self.config[\"encoder_embedding_size\"],\n",
    "            padding_idx=self.config[\"X_padding_idx\"],\n",
    "        )\n",
    "\n",
    "        if self.config[\"recurrent_layer_type\"] == \"RNN\":\n",
    "            self.recursive_layer = nn.RNN(\n",
    "                input_size=self.config[\"encoder_embedding_size\"],\n",
    "                hidden_size=self.config[\"encoder_hidden_size\"],\n",
    "                num_layers=self.config[\"num_encoder_layers\"],\n",
    "                dropout=self.config[\"encoder_dropout_prob\"],\n",
    "                bidirectional=self.config[\"encoder_bidir\"],\n",
    "                nonlinearity=self.config[\"encoder_nonlinearity\"],\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        ## Initialize H0\n",
    "        ##! The doc said the H0 will dafault to zeros. Going to check this https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n",
    "        e_x = self.embedding(x)\n",
    "\n",
    "        ## Pack the padded input for better computation\n",
    "        packed = pack_padded_sequence(e_x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        output_packed, h_n = self.recursive_layer(packed)\n",
    "\n",
    "        output, _ = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        return e_x, output, h_n\n",
    "\n",
    "\n",
    "class DecoderNetwork(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super(DecoderNetwork, self).__init__()\n",
    "        self.config = config\n",
    "        ## Tamil encoder\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"Y_vocab_size\"],\n",
    "            embedding_dim=config[\"decoder_embedding_size\"],\n",
    "            padding_idx=config[\"Y_padding_idx\"],\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=config[\"decoder_embedding_size\"],\n",
    "            hidden_size=config[\"decoder_hidden_size\"],\n",
    "            num_layers=config[\"num_decoder_layers\"],\n",
    "            dropout=config[\"decoder_dropout_prob\"],\n",
    "            bidirectional=config[\"decoder_bidir\"],\n",
    "            nonlinearity=config[\"decoder_nonlinearity\"],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(config[\"decoder_hidden_size\"], config[\"Y_vocab_size\"])\n",
    "\n",
    "    def forward(self, y_decoder_input, encoder_hidden):\n",
    "        \"\"\"\n",
    "        y_decoder_input: (batch, tgt_seq_len)\n",
    "        encoder_hidden: (num_layers * num_directions, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(y_decoder_input)  # (batch, tgt_seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(embedded, encoder_hidden)\n",
    "        # output: (batch, tgt_seq_len, hidden_size)\n",
    "        logits = self.fc(output)  # (batch, tgt_seq_len, vocab_size)\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30])\n",
      "torch.Size([2, 26])\n",
      "torch.Size([2, 26])\n"
     ]
    }
   ],
   "source": [
    "#from RecursiveNetwork import EncoderNetwork, DecoderNetwork\n",
    "train_iter = iter(train_loader)\n",
    "x,y_dec_ip,y_dec_op = next(train_iter)\n",
    "\n",
    "print(x.shape)\n",
    "print(y_dec_ip.shape)\n",
    "print(y_dec_op.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\"encoder_embedding_size\":128,\n",
    "\"X_vocab_size\":26+1, ## Here +1 is because the nn.Embedding layer throws this error \"AssertionError: Padding_idx must be within num_embeddings\"\n",
    "\"X_padding_idx\":26,\n",
    "\"recurrent_layer_type\":\"RNN\",\n",
    "\"encoder_hidden_size\":256,\n",
    "\"num_encoder_layers\":1,\n",
    "\"encoder_dropout_prob\":0.0,\n",
    "\"encoder_bidir\":False,\n",
    "\"encoder_nonlinearity\":\"tanh\",\n",
    "\"Y_vocab_size\":48+1, ## Here +1 is because the nn.Embedding layer throws this error \"AssertionError: Padding_idx must be within num_embeddings\"\n",
    "\"decoder_embedding_size\":128,\n",
    "\"Y_padding_idx\":48,\n",
    "\"decoder_hidden_size\":256,\n",
    "\"num_decoder_layers\":1,\n",
    "\"decoder_dropout_prob\":0.0,\n",
    "\"decoder_bidir\":False,\n",
    "\"decoder_nonlinearity\":\"tanh\",\n",
    "\"Y_true_vocab_size\":48, ## No need for extra digit for padding s required by nn.Embedding\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_enc_net = EncoderNetwork(config = config)\n",
    "RNN_dec_net = DecoderNetwork(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_, out, h_out = RNN_enc_net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 30, 256]) torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape, h_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0265, -1.1140, -2.0928,  ..., -1.0514,  0.8493, -1.5793],\n",
       "        [ 0.4141, -0.0973, -2.3049,  ...,  0.1408, -0.1397,  0.0620],\n",
       "        [-1.1666,  0.8203,  0.3888,  ...,  1.5981, -1.2054, -0.8678],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, hidden = RNN_dec_net.forward(y_dec_ip,encoder_hidden=h_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 26, 49])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
